{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for Collaborative Filtering (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The code below is for installing PyTorch with our specific GPU support\n",
    "\n",
    "Needs to be addapted on a different environment\n",
    "\n",
    "-Cuda compilation tools, release 12.5, V12.5.40\n",
    "\n",
    "-Build cuda_12.5.r12.5/compiler.34177558_0\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Autoencoders\n",
    "\n",
    "Autoencoders are neural networks that are trained to reconstruct their input data. They are especially powerful for tasks such as dimensionality reduction, anomaly detection, and recommendation systems. \n",
    "\n",
    "In the context of collaborative filtering, Autoencoders can:\n",
    "- Learn a **low-dimensional representation** of users and items (hotels and channels) in the hidden layers.\n",
    "- Reconstruct the interactions between users (hotels) and items (channels), allowing us to predict the interactions that have not been seen before.\n",
    "- Provide recommendations based on these reconstructions.\n",
    "\n",
    "Autoencoders help in capturing **non-linear patterns** in the data, which may not be possible with simpler linear models like SVD.\n",
    "\n",
    "### How Autoencoders Work\n",
    "\n",
    "An Autoencoder is composed of two main components:\n",
    "1. **Encoder**: This part compresses the input data into a lower-dimensional space (also known as the bottleneck layer or latent space).\n",
    "2. **Decoder**: This part reconstructs the input data from the compressed representation produced by the encoder.\n",
    "\n",
    "The architecture can be visualized as follows:\n",
    "\n",
    "$$\n",
    "\\text{Input Data} \\xrightarrow{\\text{Encoder}} \\text{Latent Representation} \\xrightarrow{\\text{Decoder}} \\text{Reconstructed Data}\n",
    "$$\n",
    "\n",
    "The goal of training an Autoencoder is to minimize the difference between the **input data** and the **reconstructed data**. This is achieved through **backpropagation** and an optimization process (e.g., Adam optimizer) using a loss function (e.g., Mean Squared Error).\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let the input data matrix be $ X \\in \\mathbb{R}^{m \\times n} $, where $ m $ is the number of hotels (users) and $ n $ is the number of channels (items). The objective of the Autoencoder is to learn an encoding function $ f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k $ that maps the input data to a lower-dimensional space of size $ k $, and a decoding function $ g: \\mathbb{R}^k \\rightarrow \\mathbb{R}^n $ that maps the latent representation back to the original space.\n",
    "\n",
    "The Autoencoder is trained by minimizing the **reconstruction error**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X, \\hat{X}) = \\frac{1}{m} \\sum_{i=1}^{m} \\| X_i - \\hat{X}_i \\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ X_i $ is the original interaction vector for hotel $ i $,\n",
    "- $ \\hat{X}_i $ is the reconstructed interaction vector for hotel $ i $,\n",
    "- $ \\|\\cdot\\|_2 $ denotes the Euclidean norm.\n",
    "\n",
    "This loss function is optimized to minimize the difference between the original and reconstructed data, ensuring that the model captures the patterns in the data.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The **encoder** is a neural network that compresses the input data into a latent representation. In our case, the input data is a one-hot encoded vector representing the interactions of a hotel with all available channels. The encoder has two layers:\n",
    "- A fully connected layer that reduces the dimensionality from the input size $ n $ (number of channels) to a smaller hidden dimension.\n",
    "- Another fully connected layer that reduces the representation further to half of the previous hidden size, providing a compact latent space.\n",
    "\n",
    "Mathematically, for an input vector $ \\mathbf{x} $, the encoder function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = f(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{h} $ is the latent representation, $ \\mathbf{W}_1 $ and $ \\mathbf{b}_1 $ are the weights and bias of the first layer, and $ f $ is an activation function (ReLU).\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The **decoder** reconstructs the input data from the latent representation. It consists of two fully connected layers:\n",
    "- The first layer expands the latent space back to a larger dimension.\n",
    "- The second layer reconstructs the original data.\n",
    "\n",
    "Mathematically, the decoder function is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = g(\\mathbf{W}_2 \\mathbf{h} + \\mathbf{b}_2)\n",
    "$$\n",
    "\n",
    "where $ \\hat{\\mathbf{x}} $ is the reconstructed data, $ \\mathbf{W}_2 $ and $ \\mathbf{b}_2 $ are the weights and bias of the second layer, and $ g $ is typically a Sigmoid activation function to ensure the output is between 0 and 1 (appropriate for binary data, like hotel-channel interactions).\n",
    "\n",
    "### Training\n",
    "\n",
    "The model is trained by feeding in the input interaction matrix (hotel-channel interaction data), which consists of binary values: 1 if a hotel has interacted with a channel, and 0 if not. The network tries to predict the missing values in the interaction matrix (i.e., recommending new channels).\n",
    "\n",
    "The training objective is to minimize the **Mean Squared Error** (MSE) between the original input and the reconstructed data. Once trained, the model can be used to predict the interaction matrix for hotels and generate recommendations based on the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data \n",
    "\n",
    "# Information about individual channels\n",
    "data_lake_prd_314410_cz_canais = pd.read_csv('../data/lookups/data-lake-prd-314410.cz.canais.csv')\n",
    "\n",
    "# List of hotel-channel combinations as of January 2025\n",
    "hotel_city_chanel_combin_extract  = pd.read_csv('../data/other/hotel_city_chanel_combin_extract.csv')\n",
    "hotel_city_chanel_combin_extract.dropna(inplace=True)\n",
    "hotel_city_chanel_combin_extract.drop(columns=['Cidade_ID'], inplace=True)\n",
    "hotel_city_chanel_combin_extract.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is detected\n",
    "print(torch.cuda.get_device_name(0))  # Should print your GPU model\n",
    "\n",
    "#2.5.1+cu121\n",
    "#True\n",
    "#NVIDIA RTX A2000 Laptop GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset (long form list of hotels-channels combinations) to a new DataFrame\n",
    "df = hotel_city_chanel_combin_extract.copy()\n",
    "\n",
    "# Map Hotel_IDs and Channel_IDs to integer indices\n",
    "hotel_ids = df[\"Hotel_ID\"].unique()\n",
    "channel_ids = df[\"Canal_ID\"].unique()\n",
    "\n",
    "hotel_to_idx = {hotel: i for i, hotel in enumerate(hotel_ids)}\n",
    "channel_to_idx = {channel: i for i, channel in enumerate(channel_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction matrix (hotels Ã— channels)\n",
    "num_hotels = len(hotel_ids)\n",
    "num_channels = len(channel_ids)\n",
    "interaction_matrix = np.zeros((num_hotels, num_channels)) # hotel rows x channel columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_matrix \n",
    "for _, row in df.iterrows():\n",
    "    h_idx = hotel_to_idx[row[\"Hotel_ID\"]]\n",
    "    c_idx = channel_to_idx[row[\"Canal_ID\"]]\n",
    "    interaction_matrix[h_idx, c_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "train_data, test_data = train_test_split(interaction_matrix, test_size=0.2, random_state=42)    \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_tensor = torch.FloatTensor(train_data)\n",
    "test_tensor = torch.FloatTensor(test_data)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_tensor), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid() # Outputs between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "input_dim = num_channels # Each row is a hotel, so input_dim = number of channels (\"columns\")\n",
    "autoencoder = Autoencoder(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder\n",
    "\n",
    "'''\n",
    "Autoencoder(\n",
    "  (encoder): Sequential(\n",
    "    (0): Linear(in_features=732, out_features=128, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
    "    (3): ReLU()\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=128, out_features=732, bias=True)\n",
    "    (3): Sigmoid()\n",
    "  )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Autoencoder\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    autoencoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch[0]  # Extract input tensor\n",
    "        outputs = autoencoder(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, inputs)  # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "autoencoder.eval()\n",
    "hotel_recommendations = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(torch.FloatTensor(interaction_matrix)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hotel_idx, scores in enumerate(reconstructed):\n",
    "    sorted_channels = np.argsort(scores)[::-1]  # Sort channels by predicted score (descending)\n",
    "    recommended_channels = [channel_ids[i] for i in sorted_channels[:50]]  # Top 50 channels\n",
    "    hotel_recommendations[hotel_ids[hotel_idx]] = recommended_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show recommendations for a few hotels\n",
    "for hotel, recs in list(hotel_recommendations.items())[:5]:\n",
    "    print(f\"Hotel {hotel} -> Recommended Channels: {recs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_recommendations.get(16573.0, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to DataFrame\n",
    "df_recommendations = pd.DataFrame(\n",
    "    [(hotel, channel) for hotel, channels in hotel_recommendations.items() for channel in channels],\n",
    "    columns=['Hotel_ID', 'Channel_ID']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendations[\"Hotel_ID\"] = df_recommendations[\"Hotel_ID\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_channels = hotel_city_chanel_combin_extract.groupby(\"Hotel_ID\")[\"Canal_ID\"].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out already existing channels\n",
    "filtered_recommendations = {\n",
    "    hotel: [channel for channel in channels if channel not in existing_channels.get(hotel, set())]\n",
    "    for hotel, channels in hotel_recommendations.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_filtered_recommendations = pd.DataFrame([\n",
    "    (hotel, channel) for hotel, channels in filtered_recommendations.items() for channel in channels\n",
    "], columns=[\"Hotel_ID\", \"Recommended_Channel\"])\n",
    "\n",
    "df_filtered_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_recommendations.to_csv(\"../out/autoencoder_hotel_channel_recommendations_top50.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is in evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "# Pass the full dataset through the encoder to get latent features\n",
    "with torch.no_grad():\n",
    "    encoded_hotels = autoencoder.encoder(torch.FloatTensor(interaction_matrix)).numpy()  # Extract the encoded representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity between hotels based on their latent representations\n",
    "similarity_matrix = cosine_similarity(encoded_hotels)\n",
    "\n",
    "# Get the most similar hotels for a given hotel\n",
    "hotel_id = 11332  # Example hotel\n",
    "\n",
    "if hotel_id in hotel_to_idx:  # Ensure the hotel exists in our mapping\n",
    "    similar_hotels = similarity_matrix[hotel_to_idx[hotel_id]].argsort()[-10:][::-1]\n",
    "    print(\"Most similar hotels:\", similar_hotels)\n",
    "else:\n",
    "    print(f\"Hotel ID {hotel_id} not found in hotel_to_idx.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the channel IDs for each hotel\n",
    "channels_hotel_11332 = set(df_filtered_recommendations[df_filtered_recommendations[\"Hotel_ID\"] == 11332][\"Recommended_Channel\"])\n",
    "channels_hotel_13558 = set(df_filtered_recommendations[df_filtered_recommendations[\"Hotel_ID\"] == 13558][\"Recommended_Channel\"])\n",
    "\n",
    "# Compute the intersection of the two sets (overlap)\n",
    "overlap_channels = channels_hotel_11332.intersection(channels_hotel_13558)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Channels for Hotel 11332: {channels_hotel_11332}\")\n",
    "print(f\"Channels for Hotel 13558: {channels_hotel_13558}\")\n",
    "print(f\"Overlap Channels: {overlap_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Autoencoders compress the hotel-channel interaction matrix into a lower-dimensional latent space (encoder), to reconstruct the interaction matrix from that latent representation (decoder) and to minimize reconstruction error. \n",
    "These means that Autoencoders learn to \"rebuild\" the original matrix rather than directly capturing high-variance structures (as SVD does).\n",
    "\n",
    "- In SVD, hotels with similar interaction patterns tend to get similar recommendations, because the singular vectors capture global patterns in the data. \n",
    "- In Autoencoders, the learned latent space might not be structured in the same way, so similar hotels might have very different activations in the bottleneck layer.\n",
    "\n",
    "As such, we prefer to exploy methologies for capturing high-variance structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
